# LocalStack and AWS
# AWS_REGION: AWS region string for boto3 clients and LocalStack, for example us-east-1.
AWS_REGION=us-east-1
# AWS_ACCESS_KEY_ID: AWS credential used by boto3 and LocalStack, any string for LocalStack.
AWS_ACCESS_KEY_ID=test
# AWS_SECRET_ACCESS_KEY: AWS credential secret used by boto3 and LocalStack, any string for LocalStack.
AWS_SECRET_ACCESS_KEY=test
# AWS_SESSION_TOKEN: Optional AWS session token for temporary credentials, blank if unused.
AWS_SESSION_TOKEN=test
# S3_BUCKET: S3 bucket name for parsed text, state blobs, traces, and caches.
S3_BUCKET=rlm-local
# DDB_TABLE_PREFIX: Prefix for DynamoDB tables, for example rlm creates rlm_sessions and others.
DDB_TABLE_PREFIX=rlm
# LOCALSTACK_ENDPOINT_URL: Endpoint URL for S3 and DynamoDB in LocalStack. Containers use http://localstack:4566.
# Host tools typically use http://localhost:4566.
LOCALSTACK_ENDPOINT_URL=http://localstack:4566
# EXTRA_CORS_ALLOWED_ORIGINS: Comma-separated origins for browser access to LocalStack.
EXTRA_CORS_ALLOWED_ORIGINS=http://localhost:3000
# EXTRA_CORS_ALLOWED_HEADERS: Additional CORS headers for LocalStack (use * for all).
EXTRA_CORS_ALLOWED_HEADERS=*
# EXTRA_CORS_EXPOSE_HEADERS: Additional exposed headers for LocalStack (use * for all).
EXTRA_CORS_EXPOSE_HEADERS=*
# AWS_ENDPOINT_URL: Alias fallback for LOCALSTACK_ENDPOINT_URL when LOCALSTACK_ENDPOINT_URL is unset.
AWS_ENDPOINT_URL=http://localstack:4566

# API
# API_HOST: Uvicorn bind host for the API service, 0.0.0.0 for Docker.
API_HOST=0.0.0.0
# API_PORT: Uvicorn bind port for the API service and container port mapping.
API_PORT=8080
# API_KEY_PEPPER: HMAC pepper for API key hashing, must match the value used to seed rlm_api_keys.
API_KEY_PEPPER=smoke-pepper
# REQUEST_SIZE_LIMIT_BYTES: Integer request body limit in bytes, blank or 0 disables the limit.
REQUEST_SIZE_LIMIT_BYTES=
# ENABLE_METRICS: true or false, enables /metrics if prometheus-client is installed.
ENABLE_METRICS=false
# ENABLE_OTEL_TRACING: true or false, enables OTLP tracing in FastAPI if opentelemetry is installed.
# OTEL_SERVICE_NAME can optionally set the service name for traces.
ENABLE_OTEL_TRACING=false
# ENABLE_RETURN_TRACE: true or false, allows execution options.return_trace to return trace_s3_uri.
ENABLE_RETURN_TRACE=false

# Parser service
# PARSER_HOST: Uvicorn bind host for the parser service, 0.0.0.0 for Docker.
PARSER_HOST=0.0.0.0
# PARSER_PORT: Uvicorn bind port for the parser service and container port mapping.
PARSER_PORT=8081
# PARSER_SERVICE_URL: URL ingestion workers call to reach the parser service.
PARSER_SERVICE_URL=http://rlm-parser:8081
# PARSER_SERVICE_AUTH_SECRET_ARN: Reserved for parser auth secret lookup, currently unused.
PARSER_SERVICE_AUTH_SECRET_ARN=

# Sandbox runner
# SANDBOX_RUNNER: local or lambda, controls how sandbox steps are executed.
SANDBOX_RUNNER=local
# SANDBOX_LAMBDA_FUNCTION_NAME: Lambda function name for sandbox steps when SANDBOX_RUNNER=lambda.
SANDBOX_LAMBDA_FUNCTION_NAME=rlm-sandbox-step
# SANDBOX_LAMBDA_TIMEOUT_SECONDS: Optional client timeout for Lambda invokes.
SANDBOX_LAMBDA_TIMEOUT_SECONDS=

# Worker selection
# WORKER_MODE: ingestion or orchestrator, selects worker entrypoint behavior.
WORKER_MODE=orchestrator
# INGESTION_BATCH_LIMIT: Integer max documents processed per ingestion poll loop.
INGESTION_BATCH_LIMIT=10
# ORCHESTRATOR_BATCH_LIMIT: Integer max executions processed per orchestrator poll loop.
ORCHESTRATOR_BATCH_LIMIT=1
# WORKER_POLL_INTERVAL_SECONDS: Float sleep interval when no work is found.
WORKER_POLL_INTERVAL_SECONDS=0.5

# Models and budgets
# DEFAULT_ROOT_MODEL: Root model name used when no models are provided by session or request.
DEFAULT_ROOT_MODEL=gpt-5
# DEFAULT_SUB_MODEL: Sub model name used when no models are provided, enables subcalls when set.
DEFAULT_SUB_MODEL=gpt-5-nano
# DEFAULT_BUDGETS_JSON: JSON object for Budgets fields: max_turns, max_total_seconds, max_step_seconds,
# max_spans_total, max_spans_per_step, max_tool_requests_per_step, max_llm_subcalls,
# max_llm_prompt_chars, max_total_llm_prompt_chars, max_stdout_chars, max_state_chars. Used when no
# budgets are provided by session or request, blank disables defaults.
DEFAULT_BUDGETS_JSON=
# DEFAULT_MODELS_JSON: JSON object with root_model and sub_model, used when no models are provided and
# overrides DEFAULT_ROOT_MODEL and DEFAULT_SUB_MODEL. Blank disables defaults.
DEFAULT_MODELS_JSON=
# MODEL_CONTEXT_WINDOWS_JSON: JSON object mapping model names to context window sizes for baseline fit
# checks. Blank disables defaults.
MODEL_CONTEXT_WINDOWS_JSON={"gpt-5": 400000, "gpt-5-nano": 400000}

# Provider settings
# LLM_PROVIDER: fake or openai, selects orchestrator provider implementation. Any other value is invalid.
LLM_PROVIDER=openai
# LLM_PROVIDER_SECRET_ARN: Reserved for provider secret lookup, currently unused.
LLM_PROVIDER_SECRET_ARN=
# OPENAI_API_KEY: OpenAI API key, required when LLM_PROVIDER=openai.
OPENAI_API_KEY=
# OPENAI_BASE_URL: Override OpenAI base URL, blank uses default.
OPENAI_BASE_URL=
# OPENAI_TIMEOUT_SECONDS: Float seconds for OpenAI client timeout, blank uses 30.0.
OPENAI_TIMEOUT_SECONDS=
# OPENAI_MAX_RETRIES: Integer retry attempts for OpenAI provider, blank uses 3.
OPENAI_MAX_RETRIES=
# EVAL_JUDGE_PROVIDER: Provider name for the evaluation judge LLM (openai).
EVAL_JUDGE_PROVIDER=openai
# EVAL_JUDGE_MODEL: Model name for the evaluation judge LLM, blank disables judge runs.
EVAL_JUDGE_MODEL=

# Feature flags and search
# ENABLE_SEARCH_DEFAULT: true or false, default session option for search and indexing.
ENABLE_SEARCH_DEFAULT=false
# ENABLE_MCP: Reserved for MCP server toggle, currently unused by code.
ENABLE_MCP=false
# ENABLE_TRACE_REDACTION: true or false, gates execution options.redact_trace.
ENABLE_TRACE_REDACTION=false
# ENABLE_ROOT_STATE_SUMMARY: true or false, include safe state summary in root prompt inputs.
ENABLE_ROOT_STATE_SUMMARY=false
# ENABLE_EVAL_JUDGE: true or false, runs Ragas judge metrics after answerer completion.
ENABLE_EVAL_JUDGE=false
# VERIFY_S3_OBJECTS_FOR_READINESS: true or false, verify parsed S3 objects before marking sessions ready.
VERIFY_S3_OBJECTS_FOR_READINESS=false
# TOOL_RESOLUTION_MAX_CONCURRENCY: Max concurrent tool resolutions per step.
TOOL_RESOLUTION_MAX_CONCURRENCY=4
# SEARCH_BACKEND_CONFIG: JSON object for search indexing and cache settings: chunk_size_chars,
# chunk_overlap_chars, index_prefix, cache_prefix. Used by ingestion indexing and search cache,
# blank uses defaults.
SEARCH_BACKEND_CONFIG=
# RATE_LIMITS_JSON: JSON object for API rate limiting, either {"max_requests":..., "window_seconds":...}
# or {"default": {...}, "tenants": {"tenant_id": {...}}}. Blank disables rate limiting.
RATE_LIMITS_JSON=

# UI proxy targets (Next.js rewrites)
# API_PROXY_TARGET: Internal API base URL for UI proxying. Use http://rlm-api:8080 in Docker.
API_PROXY_TARGET=http://rlm-api:8080
# LOCALSTACK_PROXY_TARGET: Internal LocalStack base URL for UI proxying. Use http://localstack:4566 in Docker.
LOCALSTACK_PROXY_TARGET=http://localstack:4566
